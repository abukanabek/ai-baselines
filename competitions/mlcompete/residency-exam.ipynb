{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13929266,"sourceType":"datasetVersion","datasetId":8876412}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://platform.olimpiada-ai.ro/problems/43\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport torch\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:45:32.443297Z","iopub.execute_input":"2025-11-30T10:45:32.444239Z","iopub.status.idle":"2025-11-30T10:45:36.105710Z","shell.execute_reply.started":"2025-11-30T10:45:32.444205Z","shell.execute_reply":"2025-11-30T10:45:36.104854Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:45:36.107110Z","iopub.execute_input":"2025-11-30T10:45:36.107521Z","iopub.status.idle":"2025-11-30T10:45:36.167247Z","shell.execute_reply.started":"2025-11-30T10:45:36.107495Z","shell.execute_reply":"2025-11-30T10:45:36.166389Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/residency-exam/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/residency-exam/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:45:36.168146Z","iopub.execute_input":"2025-11-30T10:45:36.168410Z","iopub.status.idle":"2025-11-30T10:45:36.864044Z","shell.execute_reply.started":"2025-11-30T10:45:36.168387Z","shell.execute_reply":"2025-11-30T10:45:36.863215Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:45:37.962729Z","iopub.execute_input":"2025-11-30T10:45:37.963039Z","iopub.status.idle":"2025-11-30T10:45:37.986538Z","shell.execute_reply.started":"2025-11-30T10:45:37.962985Z","shell.execute_reply":"2025-11-30T10:45:37.985651Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                               SampleID  \\\n0  e9ad821a-c438-4965-9f77-760819dfa155   \n1  e3d3c4e1-4fb2-45e7-9f88-247cc8f373b3   \n2  dc6794a3-b108-47c5-8b1b-3b4931577249   \n\n                                            Question      Option0     Option1  \\\n0  Chronic urethral obstruction due to benign pri...  Hyperplasia   Hyperophy   \n1  Which vitamin is supplied from only animal sou...    Vitamin C  Vitamin B7   \n2   Growth hormone has its effect on growth through?     Directly       IG1-1   \n\n       Option2                 Option3  Answer  \n0      Atrophy                Dyplasia       2  \n1  Vitamin B12               Vitamin D       2  \n2    Thyroxine  Intranuclear receptors       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SampleID</th>\n      <th>Question</th>\n      <th>Option0</th>\n      <th>Option1</th>\n      <th>Option2</th>\n      <th>Option3</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e9ad821a-c438-4965-9f77-760819dfa155</td>\n      <td>Chronic urethral obstruction due to benign pri...</td>\n      <td>Hyperplasia</td>\n      <td>Hyperophy</td>\n      <td>Atrophy</td>\n      <td>Dyplasia</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>e3d3c4e1-4fb2-45e7-9f88-247cc8f373b3</td>\n      <td>Which vitamin is supplied from only animal sou...</td>\n      <td>Vitamin C</td>\n      <td>Vitamin B7</td>\n      <td>Vitamin B12</td>\n      <td>Vitamin D</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dc6794a3-b108-47c5-8b1b-3b4931577249</td>\n      <td>Growth hormone has its effect on growth through?</td>\n      <td>Directly</td>\n      <td>IG1-1</td>\n      <td>Thyroxine</td>\n      <td>Intranuclear receptors</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"!pip install llama-cpp-python -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:45:39.492029Z","iopub.execute_input":"2025-11-30T10:45:39.492848Z","iopub.status.idle":"2025-11-30T10:47:41.353875Z","shell.execute_reply.started":"2025-11-30T10:45:39.492823Z","shell.execute_reply":"2025-11-30T10:47:41.352909Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(\n    repo_id='mradermacher/MedicalQwen3-Reasoning-4B-i1-GGUF',\n    filename='MedicalQwen3-Reasoning-4B.i1-Q4_K_M.gguf',\n    local_dir='medicalqwen'\n)\n\nmodel_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:47:41.355533Z","iopub.execute_input":"2025-11-30T10:47:41.355783Z","iopub.status.idle":"2025-11-30T10:47:47.314378Z","shell.execute_reply.started":"2025-11-30T10:47:41.355759Z","shell.execute_reply":"2025-11-30T10:47:47.313779Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"MedicalQwen3-Reasoning-4B.i1-Q4_K_M.gguf:   0%|          | 0.00/2.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c66cb1f6d9141769bdc5f73893894f3"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'medicalqwen/MedicalQwen3-Reasoning-4B.i1-Q4_K_M.gguf'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:47:52.652512Z","iopub.execute_input":"2025-11-30T10:47:52.653285Z","iopub.status.idle":"2025-11-30T10:47:52.859618Z","shell.execute_reply.started":"2025-11-30T10:47:52.653248Z","shell.execute_reply":"2025-11-30T10:47:52.858355Z"}},"outputs":[{"name":"stdout","text":"Sun Nov 30 10:47:52 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   32C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from llama_cpp import Llama\n\nllm = Llama(\n    model_path=model_path,\n    n_threads=4,\n    verbose=True,\n    n_gpu_layers=1\n)\n\n# llm(\"What are rabies?\", stop=['\\n'])['choices'][0]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:49:42.164990Z","iopub.execute_input":"2025-11-30T10:49:42.165756Z","iopub.status.idle":"2025-11-30T10:49:44.999938Z","shell.execute_reply.started":"2025-11-30T10:49:42.165727Z","shell.execute_reply":"2025-11-30T10:49:44.998930Z"}},"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 51 key-value pairs and 398 tensors from medicalqwen/MedicalQwen3-Reasoning-4B.i1-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = MedicalQwen3 Reasoning 4B\nllama_model_loader: - kv   3:                           general.basename str              = MedicalQwen3-Reasoning\nllama_model_loader: - kv   4:                         general.size_label str              = 4B\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\nllama_model_loader: - kv   6:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   7:                  general.base_model.0.name str              = Qwen3 4B\nllama_model_loader: - kv   8:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-4B\nllama_model_loader: - kv  10:                      general.dataset.count u32              = 2\nllama_model_loader: - kv  11:                     general.dataset.0.name str              = Medical O1 Reasoning SFT\nllama_model_loader: - kv  12:             general.dataset.0.organization str              = FreedomIntelligence\nllama_model_loader: - kv  13:                 general.dataset.0.repo_url str              = https://huggingface.co/FreedomIntelli...\nllama_model_loader: - kv  14:                     general.dataset.1.name str              = Medical Instruction 120k\nllama_model_loader: - kv  15:             general.dataset.1.organization str              = Mohammed Altaf\nllama_model_loader: - kv  16:                 general.dataset.1.repo_url str              = https://huggingface.co/Mohammed-Altaf...\nllama_model_loader: - kv  17:                               general.tags arr[str,8]       = [\"text-generation-inference\", \"transf...\nllama_model_loader: - kv  18:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv  19:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv  20:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv  21:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  22:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  23:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  24:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  25:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  26:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  27:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  28:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  29:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  30:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  31:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  32:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  33:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 151654\nllama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  38:               general.quantization_version u32              = 2\nllama_model_loader: - kv  39:                          general.file_type u32              = 15\nllama_model_loader: - kv  40:                                general.url str              = https://huggingface.co/mradermacher/M...\nllama_model_loader: - kv  41:              mradermacher.quantize_version str              = 2\nllama_model_loader: - kv  42:                  mradermacher.quantized_by str              = mradermacher\nllama_model_loader: - kv  43:                  mradermacher.quantized_at str              = 2025-11-30T02:54:37+01:00\nllama_model_loader: - kv  44:                  mradermacher.quantized_on str              = marco\nllama_model_loader: - kv  45:                         general.source.url str              = https://huggingface.co/Cannae-AI/Medi...\nllama_model_loader: - kv  46:                  mradermacher.convert_type str              = hf\nllama_model_loader: - kv  47:                      quantize.imatrix.file str              = MedicalQwen3-Reasoning-4B-i1-GGUF/Med...\nllama_model_loader: - kv  48:                   quantize.imatrix.dataset str              = imatrix-training-full-3\nllama_model_loader: - kv  49:             quantize.imatrix.entries_count u32              = 252\nllama_model_loader: - kv  50:              quantize.imatrix.chunks_count u32              = 318\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 151660 '<|fim_middle|>' is not marked as EOG\nload: control token: 151659 '<|fim_prefix|>' is not marked as EOG\nload: control token: 151653 '<|vision_end|>' is not marked as EOG\nload: control token: 151648 '<|box_start|>' is not marked as EOG\nload: control token: 151646 '<|object_ref_start|>' is not marked as EOG\nload: control token: 151649 '<|box_end|>' is not marked as EOG\nload: control token: 151655 '<|image_pad|>' is not marked as EOG\nload: control token: 151651 '<|quad_end|>' is not marked as EOG\nload: control token: 151647 '<|object_ref_end|>' is not marked as EOG\nload: control token: 151652 '<|vision_start|>' is not marked as EOG\nload: control token: 151654 '<|vision_pad|>' is not marked as EOG\nload: control token: 151656 '<|video_pad|>' is not marked as EOG\nload: control token: 151644 '<|im_start|>' is not marked as EOG\nload: control token: 151661 '<|fim_suffix|>' is not marked as EOG\nload: control token: 151650 '<|quad_start|>' is not marked as EOG\nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 40960\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 40960\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = MedicalQwen3 Reasoning 4B\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 11 ','\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151654 '<|vision_pad|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU, is_swa = 0\nload_tensors: layer   1 assigned to device CPU, is_swa = 0\nload_tensors: layer   2 assigned to device CPU, is_swa = 0\nload_tensors: layer   3 assigned to device CPU, is_swa = 0\nload_tensors: layer   4 assigned to device CPU, is_swa = 0\nload_tensors: layer   5 assigned to device CPU, is_swa = 0\nload_tensors: layer   6 assigned to device CPU, is_swa = 0\nload_tensors: layer   7 assigned to device CPU, is_swa = 0\nload_tensors: layer   8 assigned to device CPU, is_swa = 0\nload_tensors: layer   9 assigned to device CPU, is_swa = 0\nload_tensors: layer  10 assigned to device CPU, is_swa = 0\nload_tensors: layer  11 assigned to device CPU, is_swa = 0\nload_tensors: layer  12 assigned to device CPU, is_swa = 0\nload_tensors: layer  13 assigned to device CPU, is_swa = 0\nload_tensors: layer  14 assigned to device CPU, is_swa = 0\nload_tensors: layer  15 assigned to device CPU, is_swa = 0\nload_tensors: layer  16 assigned to device CPU, is_swa = 0\nload_tensors: layer  17 assigned to device CPU, is_swa = 0\nload_tensors: layer  18 assigned to device CPU, is_swa = 0\nload_tensors: layer  19 assigned to device CPU, is_swa = 0\nload_tensors: layer  20 assigned to device CPU, is_swa = 0\nload_tensors: layer  21 assigned to device CPU, is_swa = 0\nload_tensors: layer  22 assigned to device CPU, is_swa = 0\nload_tensors: layer  23 assigned to device CPU, is_swa = 0\nload_tensors: layer  24 assigned to device CPU, is_swa = 0\nload_tensors: layer  25 assigned to device CPU, is_swa = 0\nload_tensors: layer  26 assigned to device CPU, is_swa = 0\nload_tensors: layer  27 assigned to device CPU, is_swa = 0\nload_tensors: layer  28 assigned to device CPU, is_swa = 0\nload_tensors: layer  29 assigned to device CPU, is_swa = 0\nload_tensors: layer  30 assigned to device CPU, is_swa = 0\nload_tensors: layer  31 assigned to device CPU, is_swa = 0\nload_tensors: layer  32 assigned to device CPU, is_swa = 0\nload_tensors: layer  33 assigned to device CPU, is_swa = 0\nload_tensors: layer  34 assigned to device CPU, is_swa = 0\nload_tensors: layer  35 assigned to device CPU, is_swa = 0\nload_tensors: layer  36 assigned to device CPU, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q6_K) (and 182 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\nrepack: repack tensor blk.0.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.1.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.1.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.2.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.2.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.3.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.6.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.6.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.9.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.9.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.12.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.12.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.15.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.18.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.18.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n.repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.21.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.21.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.22.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.22.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.24.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.27.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.27.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.28.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.28.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.28.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.28.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.30.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.31.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.32.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.32.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.32.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.32.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.32.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.33.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.33.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.33.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.33.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.33.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.34.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.34.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.34.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.34.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.34.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.35.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.35.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.35.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.35.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.35.ffn_up.weight with q4_K_8x8\n..................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 512\nllama_context: n_ctx_per_seq = 512\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:        CPU  output buffer size =     0.58 MiB\ncreate_memory: n_ctx = 512 (padded)\nllama_kv_cache_unified: layer   0: dev = CPU\nllama_kv_cache_unified: layer   1: dev = CPU\nllama_kv_cache_unified: layer   2: dev = CPU\nllama_kv_cache_unified: layer   3: dev = CPU\nllama_kv_cache_unified: layer   4: dev = CPU\nllama_kv_cache_unified: layer   5: dev = CPU\nllama_kv_cache_unified: layer   6: dev = CPU\nllama_kv_cache_unified: layer   7: dev = CPU\nllama_kv_cache_unified: layer   8: dev = CPU\nllama_kv_cache_unified: layer   9: dev = CPU\nllama_kv_cache_unified: layer  10: dev = CPU\nllama_kv_cache_unified: layer  11: dev = CPU\nllama_kv_cache_unified: layer  12: dev = CPU\nllama_kv_cache_unified: layer  13: dev = CPU\nllama_kv_cache_unified: layer  14: dev = CPU\nllama_kv_cache_unified: layer  15: dev = CPU\nllama_kv_cache_unified: layer  16: dev = CPU\nllama_kv_cache_unified: layer  17: dev = CPU\nllama_kv_cache_unified: layer  18: dev = CPU\nllama_kv_cache_unified: layer  19: dev = CPU\nllama_kv_cache_unified: layer  20: dev = CPU\nllama_kv_cache_unified: layer  21: dev = CPU\nllama_kv_cache_unified: layer  22: dev = CPU\nllama_kv_cache_unified: layer  23: dev = CPU\nllama_kv_cache_unified: layer  24: dev = CPU\nllama_kv_cache_unified: layer  25: dev = CPU\nllama_kv_cache_unified: layer  26: dev = CPU\nllama_kv_cache_unified: layer  27: dev = CPU\nllama_kv_cache_unified: layer  28: dev = CPU\nllama_kv_cache_unified: layer  29: dev = CPU\nllama_kv_cache_unified: layer  30: dev = CPU\nllama_kv_cache_unified: layer  31: dev = CPU\nllama_kv_cache_unified: layer  32: dev = CPU\nllama_kv_cache_unified: layer  33: dev = CPU\nllama_kv_cache_unified: layer  34: dev = CPU\nllama_kv_cache_unified: layer  35: dev = CPU\nllama_kv_cache_unified:        CPU KV buffer size =    72.00 MiB\nllama_kv_cache_unified: size =   72.00 MiB (   512 cells,  36 layers,  1/1 seqs), K (f16):   36.00 MiB, V (f16):   36.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 1\nllama_context: max_nodes = 3184\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\ngraph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1410\nllama_context: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \nModel metadata: {'quantize.imatrix.chunks_count': '318', 'quantize.imatrix.entries_count': '252', 'quantize.imatrix.file': 'MedicalQwen3-Reasoning-4B-i1-GGUF/MedicalQwen3-Reasoning-4B.imatrix.gguf', 'mradermacher.quantized_on': 'marco', 'mradermacher.quantized_by': 'mradermacher', 'general.url': 'https://huggingface.co/mradermacher/MedicalQwen3-Reasoning-4B-i1-GGUF', 'general.quantization_version': '2', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for forward_message in messages %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- set message = messages[index] %}\\n    {%- set tool_start = \\'<tool_response>\\' %}\\n    {%- set tool_start_length = tool_start|length %}\\n    {%- set start_of_message = message.content[:tool_start_length] %}\\n    {%- set tool_end = \\'</tool_response>\\' %}\\n    {%- set tool_end_length = tool_end|length %}\\n    {%- set start_pos = (message.content|length) - tool_end_length %}\\n    {%- if start_pos < 0 %}\\n        {%- set start_pos = 0 %}\\n    {%- endif %}\\n    {%- set end_of_message = message.content[start_pos:] %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in message.content %}\\n                {%- set content = (message.content.split(\\'</think>\\')|last).lstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (message.content.split(\\'</think>\\')|first).rstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (reasoning_content.split(\\'<think>\\')|last).lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}', 'tokenizer.ggml.padding_token_id': '151654', 'general.dataset.count': '2', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen3-4B', 'general.license': 'apache-2.0', 'mradermacher.quantize_version': '2', 'general.dataset.1.repo_url': 'https://huggingface.co/Mohammed-Altaf/medical-instruction-120k', 'general.file_type': '15', 'general.dataset.0.organization': 'FreedomIntelligence', 'tokenizer.ggml.eos_token_id': '151645', 'general.dataset.0.repo_url': 'https://huggingface.co/FreedomIntelligence/medical-o1-reasoning-SFT', 'general.type': 'model', 'qwen3.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'MedicalQwen3 Reasoning 4B', 'tokenizer.ggml.pre': 'qwen2', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Cannae-AI/MedicalQwen3-Reasoning-4B', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Qwen', 'general.basename': 'MedicalQwen3-Reasoning', 'general.architecture': 'qwen3', 'general.dataset.1.name': 'Medical Instruction 120k', 'mradermacher.quantized_at': '2025-11-30T02:54:37+01:00', 'general.dataset.1.organization': 'Mohammed Altaf', 'qwen3.block_count': '36', 'qwen3.attention.head_count_kv': '8', 'qwen3.context_length': '40960', 'qwen3.embedding_length': '2560', 'qwen3.rope.freq_base': '1000000.000000', 'qwen3.feed_forward_length': '9728', 'tokenizer.ggml.add_bos_token': 'false', 'general.size_label': '4B', 'qwen3.attention.head_count': '32', 'tokenizer.ggml.model': 'gpt2', 'general.dataset.0.name': 'Medical O1 Reasoning SFT', 'qwen3.attention.key_length': '128', 'quantize.imatrix.dataset': 'imatrix-training-full-3', 'general.base_model.0.name': 'Qwen3 4B', 'qwen3.attention.value_length': '128'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for forward_message in messages %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- set message = messages[index] %}\n    {%- set tool_start = '<tool_response>' %}\n    {%- set tool_start_length = tool_start|length %}\n    {%- set start_of_message = message.content[:tool_start_length] %}\n    {%- set tool_end = '</tool_response>' %}\n    {%- set tool_end_length = tool_end|length %}\n    {%- set start_pos = (message.content|length) - tool_end_length %}\n    {%- if start_pos < 0 %}\n        {%- set start_pos = 0 %}\n    {%- endif %}\n    {%- set end_of_message = message.content[start_pos:] %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}\nUsing chat eos_token: <|im_end|>\nUsing chat bos_token: ,\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:49:19.652119Z","iopub.execute_input":"2025-11-30T10:49:19.653127Z","iopub.status.idle":"2025-11-30T10:49:19.894880Z","shell.execute_reply.started":"2025-11-30T10:49:19.653098Z","shell.execute_reply":"2025-11-30T10:49:19.893919Z"}},"outputs":[{"name":"stdout","text":"Sun Nov 30 10:49:19 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   32C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def generate_prompt(df, idx):\n    row = df.iloc[idx]\n    question = row['Question']\n    options = [row[f'Option{i}'] for i in range(4)]\n\n    prompt = f\"\"\"\nYou are answering a multiple-choice medical question.\n\nYou MUST obey these rules:\n1. Do NOT write your reasoning or thinking.\n2. Do NOT explain anything.\n3. Do NOT output anything except the final letter.\n4. Your output MUST follow the format exactly:  \n   Answer: <letter>\n\nQuestion: {question}\n\nOptions:\nA) {options[0]}\nB) {options[1]}\nC) {options[2]}\nD) {options[3]}\n\nGive ONLY the final answer letter. Nothing else.\n\"\"\"\n\n    return prompt\n\n\ndef extract_idx(output):\n    match = re.search(r\"Answer:\\s*([A-D])\", output)\n    if match:\n        chosen = match.group(1)\n        idx = \"ABCD\".index(chosen)\n        return idx\n    else:\n        return np.nan","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Prompt:\\n\")\n\nprompt = generate_prompt(train, 3)\n\nprint(prompt)\n\nprint(f\"------------------------------\")\n\nprint(\"Response:\\n\")\n\nresponse = llm(prompt, stop=['\\n', '.', '(', ')', '?', '!'])['choices'][0]['text']\nidx = extract_idx(response)\n\nprint(response)\n\nprint(f\"------------------------------\")\n\nprint(f\"{idx} - Chosen\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answers = []\nnan_count = 0\n\nfor i in (pbar := tqdm(range(len(test)))):\n    prompt = generate_prompt(test, i)\n    response = llm(prompt, stop=['\\n', '.', '(', ')', '?', '!'])['choices'][0]['text']\n    idx = extract_idx(response)\n    answers.append(idx)\n    if idx is np.nan:\n        nan_count += 1\n    pbar.set_postfix({'nan_count': nan_count})\n    if (i+1)%10==0:\n        print(f\"{i+1}/{len(test)} processed\")\n\nprint(f\"Nan count: {nan_count}/{len(test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm = pd.DataFrame({\n    'DatapointID': test['SampleID'],\n    'PredictedAnswer': answers\n})\n\nsubm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fill_nans_with_zeros_subm = subm.fillna(0)\n\nfill_nans_with_zeros_subm.to_csv(\"filled_submisison.csv\", index=False)\n\nfill_nans_with_zeros_subm","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}